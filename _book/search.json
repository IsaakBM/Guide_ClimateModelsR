[{"path":"index.html","id":"welcome","chapter":"1 Welcome","heading":"1 Welcome","text":"document objective explain process calculate climate change models estimate climate change metrics.","code":""},{"path":"cmip6-models.html","id":"cmip6-models","chapter":"2 CMIP6 models","heading":"2 CMIP6 models","text":"CMIP6 stands Coupled Model Intercomparison Project (CMIP). climate models simulate physics, chemistry biology atmosphere detail (!). 2013 IPCC fifth assessment report (AR5) featured climate models CMIP5, upcoming 2021 IPCC sixth assessment report (AR6) feature new state---art CMIP6 models (info Carbon Brief website.)main goal set future climate scenarios based future concentrations greenhouse gases, aerosols climate forcings project might happen future.Schematic CMIP/CMIP6 experimental design","code":""},{"path":"cmip6-models.html","id":"most-common-climate-scenarios","chapter":"2 CMIP6 models","heading":"2.1 Most common climate scenarios","text":"CMIP5 version find RCPs (Representative Concentration Pathways) new CMIP6 version called SSPs (Shared Socio‐Economic Pathways).Overview SSPsMost commonly SSPs used:SSP1-2.6: optimistic scenario, characterised shift sustainable economy reduction inequality resulting peak radiative forcing ~3 W m-2 2100SSP2-4.5: intermediate scenario, stabilisation radiative forcing levels ~4.5 W m-2 2100SSP5-8.5: Characterised continued increase greenhouse gas emissions resulting fossil-fuel-based economy increased energy demand, radiative forcing >8.5 W m-2 2100","code":""},{"path":"cmip6-models.html","id":"how-to-download-cmip6-models","chapter":"2 CMIP6 models","heading":"2.2 How to download CMIP6 models","text":"CMIP6 models free available Earth System Grid Federation website. need account download models. Check tutorial create account.website:Click open ESGF websiteGo Nodes tab explore different ESGF-CoG nodesClick NCI link, Australia National Computational Infrastructure nodeSelect NCI node, go collection CMIP6 link. main website download CMIP6 models. left several filters can play , advice filter first variable.","code":""},{"path":"cmip6-models.html","id":"variables","chapter":"2 CMIP6 models","heading":"2.2.1 Variables","text":"range variables available GCM (General Circulation Model) outputs. tab different model variable different time scales. tabs alphabetical order. ones starting “O” Ocean, followed timescale (clim = climatology, day, dec = decade, mon = month, yr) (source: Mathematical Marine Ecology Welcome Book Chapter 9).left tab:click + variable option. Select tos (ocean temperature surface). click Searchclick + Realm option. Select ocean ocnBgChem. click Searchclick + Frequency option. Select mon. click Searchclick + Variant Label option. Select r1i1p1f1 (common ensemble). click SearchThe ensemble names “r1i1p1”, “r2i1p1”, etc. Variant Label indicate ensemble members differ initial conditions (model physics ensemble members, members initialized different initial conditions control simulation). Hence, differences ensemble members represent internal variability.click + Experiment ID option. option see every single Experiment/Simulation. example, G1/G6/G7 geoengineering climate scenarios. Go bottom Experiment ID tab click ssp126. click Searchclick + Source ID option full model list Institution ID. Let’s click ACCESS-CM2 model CSIRO. click SearchIf follow previous steps get tab result similar :download model, just click List Files tab select HTTP Download","code":""},{"path":"getting-started-with-climate-data-operators-cdo.html","id":"getting-started-with-climate-data-operators-cdo","chapter":"3 Getting Started with Climate Data Operators (CDO)","heading":"3 Getting Started with Climate Data Operators (CDO)","text":"CMIP6 models come netCDF file format usually really messy work R. example, resolution CMIP6 NOAA model (SSP1-2.6) 0.25°. Let say want download model thetao variable (sea temperature depth). size model ~80gb. really hard just read file R.intention information scripts provide basic understanding can use CDO speed-netCDF file data manipulation. info go directly Max Planck Institute CDO website","code":""},{"path":"getting-started-with-climate-data-operators-cdo.html","id":"installation-process","chapter":"3 Getting Started with Climate Data Operators (CDO)","heading":"3.1 Installation Process","text":"","code":""},{"path":"getting-started-with-climate-data-operators-cdo.html","id":"macos","chapter":"3 Getting Started with Climate Data Operators (CDO)","heading":"3.1.1 MacOS","text":"Follow instruction downloaded MacPorts. MacPorts open-source community initiative design easy--use system compiling, installing, upgrading command-line Mac operating system.MacPorts website\nMacPorts downloadAfter installation (admin rights) open terminal type:port install cdoIf don’t admin rights, open terminal type:sudo port install cdo write password","code":""},{"path":"getting-started-with-climate-data-operators-cdo.html","id":"windows-10","chapter":"3 Getting Started with Climate Data Operators (CDO)","heading":"3.1.2 Windows 10","text":"current windows 10 version(s) Microsoft includes Ubuntu 16.04 LTS embedded Linux. environment offers clean integration windows file systems opportunity install CDO via native package manager Ubuntu.Install Ubuntu app Microsoft Store application. open Ubuntu terminal type:sudo apt-get install cdo write password","code":""},{"path":"getting-started-with-climate-data-operators-cdo.html","id":"linux","chapter":"3 Getting Started with Climate Data Operators (CDO)","heading":"3.2 Linux","text":"Linux go : Linux","code":""},{"path":"getting-started-with-climate-data-operators-cdo.html","id":"ncview-a-netcdf-visual-browser","chapter":"3 Getting Started with Climate Data Operators (CDO)","heading":"3.3 Ncview: a netCDF visual browser","text":"Ncview quick visual browser allows explore netCDF files easily: ncview. ncview easy use netCDF file viewer linux OS X. can read netCDF file.install ncview, open terminal type:OS X: port install ncviewLinux: sudo apt-get install ncview","code":""},{"path":"getting-started-with-climate-data-operators-cdo.html","id":"working-with-cdo","chapter":"3 Getting Started with Climate Data Operators (CDO)","heading":"3.4 Working with CDO","text":"work CDO ncview need use terminal command line: Ubuntu app Windows Terminal OS X.ncview exampleModel details","code":"\n# Establish the primary directory (OS or Linux)\n## cd ~/data/ClimateModels/tos/ssp585/\n\n# Establish the primary directory in Windows. It should be located at \"/mnt/c/\"\n## cd ~/data/ClimateModels/tos/ssp585/\n\n# View the model with ncview\n## ncview tos_Omon_GFDL-CM4_ssp585_r1i1p1f1_gr_201501-203412.nc\n# Check the details by typing in the terminal\n## cdo -sinfov tos_Omon_GFDL-CM4_ssp585_r1i1p1f1_gr_201501-203412.nc"},{"path":"getting-started-with-climate-data-operators-cdo.html","id":"regrid-process","chapter":"3 Getting Started with Climate Data Operators (CDO)","heading":"3.4.1 Regrid process","text":"regrid netCDF file using CDO need use argument remapbil, stands bilinear interpolation (methods conservative approach). CDO Syntax works like :previous line create uniform file 1deg spatial resolution. works fine one two netCDF files. However, multiple netCDF files/models (cases CMIP6 models) best way auto process write R script calls CDO system() function.function help search netCDF file particular directory “regridding process”.Key CDO function: remapbilRun regrid R function","code":"\n# Type in the terminal\n## cdo remapbil,r360x180 tos_Omon_GFDL-CM4_ssp585_r1i1p1f1_gr_201501-203412.nc test.nc\n\n# This code was written by Isaac Brito-Morales (i.britomorales@uq.edu.au)\n# Please do not distribute this code without permission.\n# NO GUARANTEES THAT CODE IS CORRECT\n# Caveat Emptor!\n\n# Function's arguments\n# ipath: directory where the netCDF files are located\n# opath: directory to allocate the new regrided netCDF files\n# resolution = resolution for the regrid process\n\nregrid <- function(ipath, opath, resolution) {\n\n####################################################################################\n####### Defining the main packages\n####################################################################################\n  # List of pacakges that we will be used\n    list.of.packages <- c(\"doParallel\", \"parallel\", \"stringr\", \"data.table\")\n  # If is not installed, install the pacakge\n    new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\n    if(length(new.packages)) install.packages(new.packages)\n  # Load packages\n    lapply(list.of.packages, require, character.only = TRUE)\n  \n####################################################################################\n####### Getting the path and directories for the files\n####################################################################################\n  # Establish the find bash command\n    line1 <- paste(noquote(\"find\"), noquote(ipath), \"-type\", \"f\", \"-name\", \n                   noquote(\"*.nc\"), \"-exec\", \"ls\", \"-l\", \"{}\")\n    line2 <- paste0(\"\\\\\", \";\")\n    line3 <- paste(line1, line2)\n  # Getting a list of directories for every netCDF file\n    dir_files <- system(line3, intern = TRUE)\n    dir_nc <- strsplit(x = dir_files, split = \" \")\n    nc_list <- lapply(dir_nc, function(x){f1 <- tail(x, n = 1)})\n  # Cleaning the directories to get a final vector of directories\n    final_nc <- lapply(nc_list, function(x) {\n      c1 <- str_split(unlist(x), pattern = \"//\")\n      c2 <- paste(c1[[1]][1], c1[[1]][2], sep = \"/\")})\n    files.nc <- unlist(final_nc)\n \n####################################################################################\n####### Starting the regrid process\n#################################################################################### \n  # Resolution\n    if(resolution == \"1\") {\n      grd <- \"r360x180\"\n    } else if(resolution == \"0.5\") {\n      grd <- \"r720x360\"\n    } else if(resolution == \"0.25\") {\n      grd <- \"r1440x720\"\n    }\n    \n  # Parallel looop\n    UseCores <- 3 # we can change this number\n    cl <- makeCluster(UseCores)  \n    registerDoParallel(cl)\n    foreach(j = 1:length(files.nc), .packages = c(\"stringr\")) %dopar% {\n      # Trying to auto the name for every model\n        var_obj <- system(paste(\"cdo -showname\", files.nc[j]), intern = TRUE)\n        var_all <- str_replace_all(string = var_obj, pattern = \" \", replacement = \"_\")\n        var <- tail(unlist(strsplit(var_all, split = \"_\")), n = 1) \n      # Running CDO regrid\n        system(paste(paste(\"cdo -remapbil,\", grd, \",\", sep = \"\"), \n                     paste(\"-selname\",var, sep = \",\"), files.nc[j], \n                     paste0(opath, basename(files.nc[j])), sep = (\" \"))) # -P 2\n    }\n    stopCluster(cl)\n}\nregrid(ipath = \"/data/ClimateModels/\",\n       opath = \"/data/ClimateModelsRegrid/\",\n       resolution = \"0.5\")"},{"path":"getting-started-with-climate-data-operators-cdo.html","id":"models-with-different-ocean-depths","chapter":"3 Getting Started with Climate Data Operators (CDO)","heading":"3.4.2 Models with different ocean depths","text":"climate models build across different depths ocean. function help search netCDF file particular directory, split file different ocean depth layer (e.g., surface, epipelagic, mesopelagic, bathypelagic), merge files estimating vertical average condition.CDO functions: showname sellevel selnameKey CDO function: vertmeanRunning ocean depth layer R function :","code":"\n# This code was written by Isaac Brito-Morales (i.britomorales@uq.edu.au)\n# Please do not distribute this code without permission.\n# NO GUARANTEES THAT CODE IS CORRECT\n# Caveat Emptor!\n\n# Arguments\n# ipath: directory where the netCDF files are located\n# opath1: directory to allocate the split files\n# opath2: directory to allocate the vertical average files\n\nolayer <- function(ipath, opath1, opath2) {\n\n####################################################################################\n####### Defining the main packages (tryining to auto this)\n####################################################################################\n  # List of pacakges that we will be used\n    list.of.packages <- c(\"doParallel\", \"parallel\", \"stringr\", \"data.table\")\n  # If is not installed, install the pacakge\n   new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\n   if(length(new.packages)) install.packages(new.packages)\n  # Load packages\n    lapply(list.of.packages, require, character.only = TRUE)\n  \n####################################################################################\n####### Getting the path and directories for the files\n####################################################################################\n  # Establish the find bash command\n    line1 <- paste(noquote(\"find\"), noquote(ipath), \"-type\", \"f\", \"-name\", \n                   noquote(\"*.nc\"), \"-exec\", \"ls\", \"-l\", \"{}\")\n    line2 <- paste0(\"\\\\\", \";\")\n    line3 <- paste(line1, line2)\n  # Getting a list of directories for every netCDF file\n    dir_files <- system(line3, intern = TRUE)\n    dir_nc <- strsplit(x = dir_files, split = \" \")\n    nc_list <- lapply(dir_nc, function(x){f1 <- tail(x, n = 1)})\n  # Cleaning the directories to get a final vector of directories\n    final_nc <- lapply(nc_list, function(x) {\n      c1 <- str_split(unlist(x), pattern = \"//\")\n      c2 <- paste(c1[[1]][1], c1[[1]][2], sep = \"/\")})\n    files.nc <- unlist(final_nc)\n \n####################################################################################\n####### Filtering by layers and generating new netCDF files with outputs\n#################################################################################### \n  # Parallel looop\n    cl <- makeCluster(3)\n    registerDoParallel(cl)\n    foreach(j = 1:length(files.nc), .packages = c(\"stringr\")) %dopar% {\n      # Trying to auto the name for every model\n        var_obj <- system(paste(\"cdo -showname\", files.nc[j]), intern = TRUE)\n        var_all <- str_replace_all(string = var_obj, pattern = \" \", replacement = \"_\")\n        var <- tail(unlist(strsplit(var_all, split = \"_\")), n = 1) \n      # Defining depths\n        levels <- as.vector(system(paste(\"cdo showlevel\", files.nc[j]), intern = TRUE))\n        lev <- unlist(strsplit(levels, split = \" \"))\n        depths <- unique(lev[lev != \"\"])\n      # Some can come in cm\n        if(depths[1] >= 50) { \n          sf <- depths[as.numeric(depths) <= 500]\n          ep <- depths[as.numeric(depths) >= 0 & as.numeric(depths) <= 20000]\n          mp <- depths[as.numeric(depths) > 20000 & as.numeric(depths) <= 100000]\n          bap <- depths[as.numeric(depths) > 100000]\n        } else {\n          sf <- depths[as.numeric(depths) <= 5]\n          ep <- depths[as.numeric(depths) >= 0 & as.numeric(depths) <= 200]\n          mp <- depths[as.numeric(depths) > 200 & as.numeric(depths) <= 1000]\n          bap <- depths[as.numeric(depths) > 1000]\n        }\n      # Running CDO\n        # Surface\n          system(paste(paste(\"cdo -L -sellevel,\", \n                             paste0(sf, collapse = \",\"), \",\", sep = \"\"), \n                       paste(\"-selname,\", var, sep = \"\"), files.nc[j], \n                       paste0(opath1, \"01-sf_\", basename(files.nc[j]))))\n        # Epipelagic\n          system(paste(paste(\"cdo -L -sellevel,\", \n                             paste0(ep, collapse = \",\"), \",\", sep = \"\"), \n                       paste(\"-selname,\", var, sep = \"\"), files.nc[j], \n                       paste0(opath1, \"02-ep_\", basename(files.nc[j]))))\n        # Mesopelagic\n          system(paste(paste(\"cdo -L -sellevel,\", \n                             paste0(mp, collapse = \",\"), \",\", sep = \"\"), \n                       paste(\"-selname,\", var, sep = \"\"), files.nc[j], \n                       paste0(opath1, \"03-mp_\", basename(files.nc[j]))))\n        # Bathypelagic\n          system(paste(paste(\"cdo -L -sellevel,\", \n                             paste0(bap, collapse = \",\"), \",\", sep = \"\"), \n                       paste(\"-selname,\", var, sep = \"\"), files.nc[j], \n                       paste0(opath1, \"04-bap_\", basename(files.nc[j]))))\n    }\n    stopCluster(cl)\n    \n####################################################################################\n####### Getting the path and directories for the \"split by depth\" files\n####################################################################################\n  # Establish the find bash command\n    line1.1 <- paste(noquote(\"find\"), noquote(opath1), \"-type\", \"f\", \"-name\", \n                     noquote(\"*.nc\"), \"-exec\", \"ls\", \"-l\", \"{}\")\n    line2.1 <- paste0(\"\\\\\", \";\")\n    line3.1 <- paste(line1.1, line2.1)\n  # Getting a list of directories for every netCDF file\n    dir_files.2 <- system(line3.1, intern = TRUE)\n    dir_nc.2 <- strsplit(x = dir_files.2, split = \" \")\n    nc_list.2 <- lapply(dir_nc.2, function(x){f1 <- tail(x, n = 1)})\n  # Cleaning the directories to get a final vector of directories\n    final_nc.2 <- lapply(nc_list.2, function(x) {\n      c1 <- str_split(unlist(x), pattern = \"//\")\n      c2 <- paste(c1[[1]][1], c1[[1]][2], sep = \"/\")})\n    files.nc.2 <- unlist(final_nc.2)\n    \n####################################################################################\n####### Filtering by layers and generating the \"weighted-average depth layer\" \n#################################################################################### \n  # Parallel looop\n    cl <- makeCluster(3)\n    registerDoParallel(cl)\n    foreach(i = 1:length(files.nc.2), .packages = c(\"stringr\")) %dopar% {\n      # Running CDO\n        system(paste(paste(\"cdo -L vertmean\", sep = \"\"), files.nc.2[i], \n                     paste0(opath2, basename(files.nc.2[i])), sep = (\" \")))\n    }\n    stopCluster(cl)\n}\nolayer(ipath = \"/data/ClimateModelsRegrid/\",\n       opath1 = \"/data/ClimateModelsRegridLayer/\", \n       opath2 = \"/data/ClimateModelsRegridLayerMean/\")"},{"path":"getting-started-with-climate-data-operators-cdo.html","id":"merge-several-netcdf-files-with-cdo","chapter":"3 Getting Started with Climate Data Operators (CDO)","heading":"3.4.3 Merge several netcdf files with CDO","text":"function help merge several ocean depth layers (model) single file.Key CDO function: mergetimeRunning merge functionThe functions build based OS. Linux please check:regrid: regrid R functionocean layers: ocean layer R functionmerge: calculates merge R function","code":"\nmerge_files <- function(ipath, opath1) {\n\n####################################################################################\n####### Defining the main packages (tryining to auto this)\n####################################################################################\n  # List of pacakges that we will be used\n    list.of.packages <- c(\"doParallel\", \"parallel\", \"stringr\", \"data.table\")\n  # If is not installed, install the pacakge\n    new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\n    if(length(new.packages)) install.packages(new.packages)\n  # Load packages\n    lapply(list.of.packages, require, character.only = TRUE)\n  \n####################################################################################\n####### Getting the path and directories for the files\n####################################################################################\n  # Establish the find bash command\n    line1 <- paste(noquote(\"find\"), noquote(ipath), \"-type\", \"f\", \"-name\", \n                   noquote(\"*.nc\"), \"-exec\", \"ls\", \"-l\", \"{}\")\n    line2 <- paste0(\"\\\\\", \";\")\n    line3 <- paste(line1, line2)\n  # Getting a list of directories for every netCDF file\n    dir_files <- system(line3, intern = TRUE)\n    dir_nc <- strsplit(x = dir_files, split = \" \")\n    nc_list <- lapply(dir_nc, function(x){f1 <- tail(x, n = 1)})\n  # Cleaning the directories to get a final vector of directories\n    final_nc <- lapply(nc_list, function(x) {\n      c1 <- str_split(unlist(x), pattern = \"//\")\n      c2 <- paste(c1[[1]][1], c1[[1]][2], sep = \"/\")})\n    files.nc <- unlist(final_nc)\n \n####################################################################################\n####### Filtering by layers and generating new netCDF files with outputs\n#################################################################################### \n  # Filtering (not dplyr!) by ocean layers\n    sf <- files.nc[str_detect(string = basename(files.nc), pattern = \"01-sf*\") == TRUE]\n    ep <- files.nc[str_detect(string = basename(files.nc), pattern = \"02-ep*\") == TRUE]\n    mp <- files.nc[str_detect(string = basename(files.nc), pattern = \"03-mp*\") == TRUE]\n    bap <- files.nc[str_detect(string = basename(files.nc), pattern = \"04-bap*\") == TRUE]\n  # Defining how many models are per ocean layer \n    model_list_sf <- lapply(sf, function(x) \n      {d1 <- unlist(strsplit(x = basename(x), split = \"_\"))[4]})\n    model_list_ep <- lapply(ep, function(x) \n      {d1 <- unlist(strsplit(x = basename(x), split = \"_\"))[4]})\n    model_list_mp <- lapply(bap, function(x) \n      {d1 <- unlist(strsplit(x = basename(x), split = \"_\"))[4]})\n    model_list_bap <- lapply(bap, function(x) \n      {d1 <- unlist(strsplit(x = basename(x), split = \"_\"))[4]})\n    models <- unique(unlist(c(model_list_sf, model_list_ep, model_list_mp, model_list_bap)))\n  # Parallel looop\n    cl <- makeCluster(3)\n    registerDoParallel(cl)\n    foreach(i = 1:length(models), .packages = c(\"stringr\")) %dopar% {\n      f1 <- ep[str_detect(string = basename(sf), pattern = models[i]) == TRUE]\n      system(paste(paste(\"cdo -L mergetime\", paste0(f1, collapse = \" \"), sep = \" \"), \n                   paste0(opath1, paste(unlist(strsplit(basename(f1[1]), \"_\"))[c(1:7)], \n                                        collapse = \"_\"), \".nc\"), sep = (\" \")))\n      f2 <- ep[str_detect(string = basename(ep), pattern = models[i]) == TRUE]\n      system(paste(paste(\"cdo -L mergetime\", paste0(f2, collapse = \" \"), sep = \" \"), \n                   paste0(opath1, paste(unlist(strsplit(basename(f2[1]), \"_\"))[c(1:7)], \n                                        collapse = \"_\"), \".nc\"), sep = (\" \")))\n      f3 <- mp[str_detect(string = basename(mp), pattern = models[i]) == TRUE]\n      system(paste(paste(\"cdo -L mergetime\", paste0(f3, collapse = \" \"), sep = \" \"), \n                   paste0(opath1, paste(unlist(strsplit(basename(f3[1]), \"_\"))[c(1:7)], \n                                        collapse = \"_\"), \".nc\"), sep = (\" \")))\n      f4 <- bap[str_detect(string = basename(bap), pattern = models[i]) == TRUE]\n      system(paste(paste(\"cdo -L mergetime\", paste0(f4, collapse = \" \"), sep = \" \"), \n                   paste0(opath1, paste(unlist(strsplit(basename(f4[1]), \"_\"))[c(1:7)], \n                                        collapse = \"_\"), \".nc\"), sep = (\" \")))\n    }\n    stopCluster(cl)\n}\nmerge_files(ipath = \"/Users/bri273/Desktop/CDO/models_regrid_vertmean/\",\n            opath1 = \"/Users/bri273/Desktop/CDO/models_regrid_zmerge/\")"},{"path":"getting-started-with-climate-data-operators-cdo.html","id":"some-useful-cdo-functions","chapter":"3 Getting Started with Climate Data Operators (CDO)","heading":"3.4.4 Some useful CDO functions","text":"CDO just regridding. interesting useful functions :yearmean: calculates annual mean monthly data input netCDF fileyearmin: calculates annual min monthly data input netCDF fileyearmax: calculates annual max monthly data input netCDF fileensmean: calculates ensemble mean several netCDF files. input files different models, function estimate mean models","code":""},{"path":"netcdf-files-in-r-raster-spatial-objects.html","id":"netcdf-files-in-r-raster-spatial-objects","chapter":"4 netCDF files in R: Raster, Spatial objects","heading":"4 netCDF files in R: Raster, Spatial objects","text":"","code":""},{"path":"netcdf-files-in-r-raster-spatial-objects.html","id":"introduction","chapter":"4 netCDF files in R: Raster, Spatial objects","heading":"4.1 Introduction","text":"aim tutorial provide worked example (.e., function) transform regridded netCDF Raster object using R.","code":""},{"path":"netcdf-files-in-r-raster-spatial-objects.html","id":"data-import","chapter":"4 netCDF files in R: Raster, Spatial objects","heading":"4.2 Data import","text":"Load required packages.","code":"\n# load packages\nlibrary(raster)\nlibrary(ncdf4)\nlibrary(ncdf4.helpers)\nlibrary(PCICt)"},{"path":"netcdf-files-in-r-raster-spatial-objects.html","id":"function-to-transform-netcdf-files-into-raster-objects","chapter":"4 netCDF files in R: Raster, Spatial objects","heading":"4.3 Function to transform netCDF files into Raster objects","text":"can read netCDF using raster:::stack raster:::terra functions raster terra packages. However, function allows control outputs.","code":"\n# This code was written by Isaac Brito-Morales (i.britomorales@uq.edu.au)\n# Please do not distribute this code without permission.\n# NO GUARANTEES THAT CODE IS CORRECT\n# Caveat Emptor!\n \n  ncdf_2D_rs <- function(nc, from, to, v = \"tos\", x = \"lon\", y = \"lat\") {\n  # Extract data from the netCDF file  \n   nc <- nc_open(nc)\n   dat <- ncvar_get(nc, v) # x, y, year \n   dat[] <- dat\n   X <- dim(dat)[1]\n   Y <- dim(dat)[2]\n   tt <- nc.get.time.series(nc, v = \"time\", time.dim.name = \"time\")\n   tt <- as.POSIXct(tt)\n   tt <- as.Date(tt)\n   nc_close(nc)\n   rs <- raster(nrow = Y, ncol = X) # Make a raster with the right dims\n  # Fix orientation of original data\n    drs <- data.frame(coordinates(rs))\n  # Create Rasters Stack\n    rs_list <- list() # empty list to allocate results\n    st <- stack()\n    for (i in 1:length(tt)) {\n      dt1 <- rasterFromXYZ(cbind(drs, as.vector(dat[,, i])))\n      dt1[]<- ifelse(dt1[] <= -2, NA, dt1[]) # for some models that have weird temperatures\n      dt1[]<- ifelse(dt1[] >= 40, NA, dt1[]) # for some models that have weird temperatures\n      st <- addLayer(st, flip(dt1, 2))\n      print(paste0(i, \" of \", length(tt)))\n    }\n    names(st) <- seq(as.Date(paste(from, \"1\", \"1\", sep = \"/\")), \n                     as.Date(paste(to, \"12\", \"1\", sep = \"/\")), by = \"month\")\n    crs(st) <- \"+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0\"\n    return(st)\n    }"},{"path":"climate-change-metrics.html","id":"climate-change-metrics","chapter":"5 Climate Change Metrics","heading":"5 Climate Change Metrics","text":"","code":""},{"path":"climate-change-metrics.html","id":"introduction-1","chapter":"5 Climate Change Metrics","heading":"5.1 Introduction","text":"aim tutorial provide worked example calculate climate change metrics using CMIP6 models. climate-change metrics used example Climate Velocity Relative Climate Exposure index.dataset contains raster-stack file format .grd monthly sea surface temperature (tos) GFDL-CM4 model SSP5-5.8 emission scenario. model goes 2015 2100.","code":""},{"path":"climate-change-metrics.html","id":"data-import-1","chapter":"5 Climate Change Metrics","heading":"5.2 Data import","text":"Load required packages data.","code":"\n# load packages\n  library(raster)\n  library(VoCC)\n  library(stringr)\n  library(dplyr)"},{"path":"climate-change-metrics.html","id":"climate-velocity-vocc","chapter":"5 Climate Change Metrics","heading":"5.3 Climate Velocity (VoCC)","text":"Climate velocity vector describes speed direction point gridded map need move remain static climate space (e.g., maintain isoline given variable univariate environment) climate change. ecological perspective, climate velocity can conceptualized speed direction species need move maintain current climate conditions climate change. reason, climate velocity can considered represent potential exposure climate change faced species climate moves beyond physiological tolerance local population. See Brito-Morales et al. 2018.Install R package: GitHub RepoTo calculate climate velocity R package VoCC provides comprehensive collection functions calculate climate velocity related metrics initial formulation latest developments. See Garcia Molinos et al. 2019.","code":"\n# Load the monthly raster object\n  rs <- raster::stack(\"data/ClimateModelsRasterMonthly/tos/ssp585/tos_Omon_GFDL-CM4_ssp585_2015-2100.grd\")\n# Establish the time period of interest (if applicable)\n  from = 2020\n  to = 2100\n# Define the time period to estimate climate velocity and subset the original raster\n  names.yrs <- paste(\"X\", seq(as.Date(paste(from, \"1\", \"1\", sep = \"/\")), as.Date(paste(to, \"12\", \"1\", sep = \"/\")), by = \"month\"), sep = \"\") %>%\n    str_replace_all(pattern = \"-\", replacement = \".\")\n  rs <- raster::subset(rs, names.yrs)\n# If Raster is monthly, get annual mean\n  index <- rep(1:nlayers(rs), each = 12, length.out = nlayers(rs))\n  rs <- stackApply(x = rs, indices = index, fun = mean)\n  \n# Calculate VoCC\n  # Temporal trend (slope)\n    slp <- tempTrend(rs, th = 10)\n  # Spatial gradient (gradient)\n    grad <- spatGrad(rs, th = 0.0001, projected = FALSE)\n  # VoCC local gradient\n    vocc <- gVoCC(slp, grad)\n    vocc$voccMag[] <- ifelse(is.infinite(vocc$voccMag[]), NA, vocc$voccMag[]) # replace inf with NAs"},{"path":"climate-change-metrics.html","id":"relative-climate-exposure-rce","chapter":"5 Climate Change Metrics","heading":"5.4 Relative Climate Exposure (RCE)","text":"RCE metric developed obtain information amount exposure climate warming local populations species face relative experience variation seasonal temperatures Brito-Morales et al. 2021. RCE calculated ratio slope linear regression projected mean annual temperatures (°C yr-1) current mean seasonal temperature range (°C):","code":"\n# The current seasonal variation (could be any range)\n  from = 2016\n  to = 2021\n# Getting the years/month to calculate de RCE index\n  names.yrs <- paste(\"X\", seq(as.Date(paste(from, \"1\", \"1\", sep = \"/\")), \n                              as.Date(paste(to, \"12\", \"1\", sep = \"/\")), by = \"month\"), \n                     sep = \"\") %>%\n    str_replace_all(pattern = \"-\", replacement = \".\")\n# Read and subset the data\n  rs <- readAll(raster::stack(\"data/ClimateModelsRasterMonthly/tos/ssp585/tos_Omon_GFDL-CM4_ssp585_2015-2100.grd\")) %>% \n    subset(names.yrs)\n# Annual min and max to estimate the rage to get the RCE\n  index <- rep(1:nlayers(rs), each = 12, length.out = nlayers(rs))\n  rs_min <- stackApply(x = rs, indices = index, fun = min)\n  rs_max <- stackApply(x = rs, indices = index, fun = max)\n# Range among the period selected\n  rs_range <- rs_max - rs_min\n  rs_range_mean <- stackApply(x = rs_range, indices = nlayers(rs_range), fun = mean)\n  \n# Get the slope\n  from = 2020\n  to = 2100\n  names.yrs <- paste(\"X\", seq(as.Date(paste(from, \"1\", \"1\", sep = \"/\")), as.Date(paste(to, \"12\", \"1\", sep = \"/\")), by = \"month\"), sep = \"\") %>%\n    str_replace_all(pattern = \"-\", replacement = \".\")\n  rs <- raster::stack(\"data/ClimateModelsRasterMonthly/tos/ssp585/tos_Omon_GFDL-CM4_ssp585_2015-2100.grd\") %>% \n    subset(names.yrs)\n  index <- rep(1:nlayers(rs), each = 12, length.out = nlayers(rs))\n  rs <- stackApply(x = rs, indices = index, fun = mean)\n  slp <- ((tempTrend(rs, th = 10))*10)*8 # x10 decadal and x8 for decades (2020-2100)\n\n# Calculate the RCE\n  RCE <- abs(slp/rs1_range_mean)"},{"path":"raster-objects-and-spatial-object.html","id":"raster-objects-and-spatial-object","chapter":"6 Raster objects and Spatial object","heading":"6 Raster objects and Spatial object","text":"","code":""},{"path":"raster-objects-and-spatial-object.html","id":"introduction-2","chapter":"6 Raster objects and Spatial object","heading":"6.1 Introduction","text":"aim provide worked example intersect rasters spatial polygon objects using sf R exactextractr R packages.example used two objects.spatial polygon object: located data/PlanningUnits/PUs_WeddellSea_100km2.shp. object contains hexagonal polygons equal area. two columns: integer unique identifiers (“id”), geometry information (“geometry”)raster object: located data/VoCC/tos/voccMag_tos_GFDL-CM4_ssp585_2015-2100.grd_2050-2100_.tif. object raster .tif object climate-velocity estimates southern ocean.objective assign eash hexagon spatial polygon object climate velocity value","code":""},{"path":"raster-objects-and-spatial-object.html","id":"data-import-2","chapter":"6 Raster objects and Spatial object","heading":"6.2 Data import","text":"First, load required packages data.","code":"\n# load packages\n  library(raster)\n  library(sf)\n  library(dplyr)\n  library(exactextractr)\n  library(stringr)\n  library(nngeo)"},{"path":"raster-objects-and-spatial-object.html","id":"function-to-replace-nas-with-nearest-neighbor","chapter":"6 Raster objects and Spatial object","heading":"6.3 Function to replace NAs with nearest neighbor","text":"functions helps replace NAs nearest neighbor interpolation method.","code":"\n# Function to replace NAs with nearest neighbor. Function wrtitten by Jason Everett\n  fCheckNAs <- function(df, vari) {\n    if (sum(is.na(pull(df, !!sym(vari))))>0){ # Check if there are NAs\n      gp <- df %>%\n        mutate(isna = is.finite(!!sym(vari))) %>%\n        group_by(isna) %>%\n        group_split()\n      \n      out_na <- gp[[1]] # DF with NAs\n      out_finite <- gp[[2]] # DF without NAs\n      \n      d <- st_nn(out_na, out_finite) %>% # Get nearest neighbour\n        unlist()\n      out_na <- out_na %>%\n        mutate(!!sym(vari) := pull(out_finite, !!sym(vari))[d])\n      df <- rbind(out_finite, out_na)\n    }\n    return(df)\n  }"},{"path":"raster-objects-and-spatial-object.html","id":"raster-by-spatial-polygon-object","chapter":"6 Raster objects and Spatial object","heading":"6.4 Raster by spatial polygon object","text":"aim provide example integrate raster sf polygon spatial object","code":"\n# Reading the spatial polygon object\n  pu_region <- st_read(\"data/PlanningUnits/PUs_WeddellSea_100km2.shp\")\n  st_crs(pu_region) <- \"+proj=laea +lat_0=-90 +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\"\n  \n# Reading the raster object\n  vocc_file <- raster::raster(\"data/VoCC/tos/voccMag_tos_GFDL-CM4_ssp585_2015-2100.grd_2050-2100_.tif\")\n  crs(vocc_file) <- CRS(\"+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0\")\n  weight_rs_vocc <- raster::area(vocc_file)\n  vocc_file <- projectRaster(vocc_file, crs = CRS(\"+proj=laea +lat_0=-90 +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\"), \n                             method = \"ngb\", over = FALSE)\n  weight_vocc <- projectRaster(weight_rs_vocc, crs = CRS(\"+proj=laea +lat_0=-90 +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\"), \n                               method = \"ngb\", over = FALSE)\n  names(vocc_file) <- \"layer\"\n  \n# Getting the value by planning unit\n  vocc_bypu <- exact_extract(vocc_file, pu_region, \"weighted_mean\", weights = weight_vocc)\n  vocc_shp <- pu_region %>%\n    dplyr::mutate(vocc = vocc_bypu) %>%\n    dplyr::relocate(cellID, vocc)\n# Replace NAs with nearest neighbor\n  vocc_sfInt <- fCheckNAs(df = vocc_shp, vari = names(vocc_shp)[2]) %>%\n  dplyr::select(-isna)"},{"path":"raster-objects-and-spatial-object.html","id":"spatial-polygon-object-by-region-of-interest","chapter":"6 Raster objects and Spatial object","heading":"6.5 Spatial polygon object by region of interest","text":"aim provide simple script assign Longhurst province identifier per planning unit","code":"\n# Reading the spatial polygon object\n  pu_region <- st_read(\"data/PlanningUnits/PUs_WeddellSea_100km2.shp\") %>% \n    st_transform(crs = CRS(\"+proj=laea +lat_0=-90 +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\"))\n# Reading Longhurst Provinces Shapefile\n  bioprovince <- st_read(\"data/Boundaries/LonghurstProvinces/Longhurst_world_v4_2010.shp\") %>% \n    st_transform(crs = CRS(\"+proj=laea +lat_0=-90 +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\")) %>% \n    st_make_valid()\n# Get the Longhurst Provinces per Planning unit\n  nr <- st_nearest_feature(pu_region, bioprovince)\n  pu_region <- pu_region %>% \n    dplyr::mutate(province = paste(as.vector(bioprovince$ProvCode[nr]), prov_name, sep = \"_\")) %>% \n    dplyr::arrange(layer)"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
